## Dynamic Neural Attention using a Recurrent Weighted Average

The [*attention mechanism*](http://colah.github.io/posts/2017-03-Distill/) is one of the more exciting innovations in the field of artificial neural networks. In a nutshell, the attention mechanism is nothing more than a weighted average.

Suppose we wanted to model time-series data using a recurrent neural network (RNN) with attention. At each processing step, the output of the RNN is weighted by an attention model. The weighted outputs are then aggregated together into a weighted average. The result from the weighted average is called a *context vector*. The context vector represents information aggregated together across any timepoints in the data.

A major constraint with the attention mechanism is that only one context vector is produced for the entire time-series data. The entire sequence of data must be read into the model before the context vector can be generated. In other words, the attention mechanism is static. To overcome this limitation, we recently proposed a new approach to compute the attention mechanism - using a moving average. Because the attention mechanism is nothing more than a weighted average, it can be computed as a running calculation. This requires saving the numerator and denominator from each processing step to use in the next iteration. By maintaining a moving average of the attention mechanism, a new context vector is produced at every timestep. With this approach, the attention mechanism becomes dynamic and can be computed on the fly.

We decided to take our approach one step further. We realized that the output of the attention mechanism could be feed back into the attention mechanism at the next processing step. The resulting model represents a new kind of RNN model. Given that the weighted average is recursively defined, we decided to call this approach a recurrent weighted average (RWA) model.

We donâ€™t expect the RWA model to always outperform alternative RNN models like the LSTM. When recent information is more important than old information, the LSTM model may be better choice. That said, there are many problems where we may want a RNN model with memory into the deep past, which is where we except to the RWA model to outperform alternative methods.

You can find a detailed description of what we did in our [arxiv manuscript](https://arxiv.org/abs/1703.01253). I hope you find our work useful. We are certainly excited about it.

-- Jared Ostmeyer
